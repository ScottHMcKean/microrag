{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes over a qlora example with a baby LLM (and my baby GPU) to get a feel for CUDA quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom utility functions\n",
    "from src.torch_utils import gpu_summary, clear_gpu\n",
    "from src.quantize import quantize, unquantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smckean/Repos/microrag/microrag_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "model_name = 'bigscience/bloomz-560m'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    config=bnb_config,\n",
    "    device_map = {\"\": 0}\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many wheels does a car have? four'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(\"How many wheels does a car have?\", return_tensors='pt').to('cuda')\n",
    "tokenized_output = foundation_model.generate(\n",
    "    input_ids=input['input_ids'],\n",
    "    attention_mask=input['attention_mask'],\n",
    "    max_new_tokens=100,\n",
    "    early_stopping=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "#Create the Dataset to create prompts.\n",
    "data = load_dataset(dataset)\n",
    "\n",
    "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "\n",
    "del data\n",
    "train_sample = train_sample.remove_columns('act')\n",
    "\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix. It seems that as higher more weight have the new training.\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
    "    bias=\"none\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "output_dir = '/home/smckean/Produced/peft_poc_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smckean/Repos/microrag/microrag_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/smckean/Repos/microrag/microrag_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments # , Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=foundation_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"prompt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 500/500 [01:20<00:00,  5.93it/s]/home/smckean/Repos/microrag/microrag_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9028, 'grad_norm': 3.9721927642822266, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 500/500 [01:21<00:00,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 81.5281, 'train_samples_per_second': 6.133, 'train_steps_per_second': 6.133, 'train_loss': 2.90278466796875, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=2.90278466796875, metrics={'train_runtime': 81.5281, 'train_samples_per_second': 6.133, 'train_steps_per_second': 6.133, 'total_flos': 84255525519360.0, 'train_loss': 2.90278466796875, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smckean/Repos/microrag/microrag_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "peft_model_path = os.path.join(output_dir, f\"lora_model\")\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [07:13<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
    "bnb_config2 = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    peft_model_path,\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    is_trainable=False,\n",
    "    #load_in_4bit=True,\n",
    "    quantization_config=bnb_config2,\n",
    "    device_map = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as a motivational coach.  I will provide some information\n",
      "about myself and what i can do in order for my clients achieve their goals. My\n",
      "first step is giving them instructions on how they should approach the\n",
      "challenges of life, such as:  \"I need help figuring out if this job could be\n",
      "better suited than other jobs available at work; it would also make me feel more\n",
      "confident when applying...\" \"My client needs advice regarding improving his\n",
      "current financial situation so that he may increase income while still being\n",
      "able affording luxury cars ... \"Your goal here are two things - both achievable\n",
      "but not mutually exclusive: 1) To motivate your clientele through effective\n",
      "communication skills 2)...\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer(\"I want you to act as a motivational coach. \", return_tensors='pt').to('cuda')\n",
    "tokenized_output = foundation_model.generate(\n",
    "    input_ids=input['input_ids'],\n",
    "    attention_mask=input['attention_mask'],\n",
    "    max_new_tokens=1000,\n",
    "    early_stopping=False,\n",
    "    repetition_penalty=3.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
    "wrapper = textwrap.TextWrapper(width=80)  # replace 80 with your desired line width\n",
    "wrapped_output = wrapper.fill(output)\n",
    "\n",
    "print(wrapped_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want you to act as a motivational coach.  I will provide some information\n",
      "about your subject matter and ask the reader how they can improve their\n",
      "performance in order that it becomes easier for them, \"I\" or \"the other person\n",
      "who is doing this task at me\"\". The purpose of my suggestion should be \"to help\n",
      "people feel more confident themselves by improving existing habits which are\n",
      "causing problems such as: stress levels anxiety depression etc., so these issues\n",
      "may not seem like too big an issue but instead become something less than\n",
      "\"outrageous\"; however difficult those changes might appear.   My first request\n",
      "would be: “I need someone else with experience helping individuals overcome\n",
      "challenges related to: health conditions/mental illnesses / personality\n",
      "disorders”  “Personal development needs improvement”; “The effects on others’\n",
      "well-being have been studied; therefore there has also recently emerged interest\n",
      "from academics regarding possible solutions ... ” \"My goal here could involve\n",
      "providing suggestions based upon current research findings and/or practical\n",
      "experiences gained during training sessions....“\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer(\"I want you to act as a motivational coach. \", return_tensors='pt').to('cuda')\n",
    "tokenized_output = loaded_model.generate(\n",
    "    input_ids=input['input_ids'],\n",
    "    attention_mask=input['attention_mask'],\n",
    "    max_new_tokens=1000,\n",
    "    early_stopping=False,\n",
    "    repetition_penalty=3.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = tokenizer.decode(tokenized_output[0], skip_special_tokens=True)\n",
    "wrapper = textwrap.TextWrapper(width=80)  # replace 80 with your desired line width\n",
    "wrapped_output = wrapper.fill(output)\n",
    "\n",
    "print(wrapped_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microrag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
